{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5351672172546387\n"
     ]
    }
   ],
   "source": [
    "from dataloader import load_citation, load_citation_v2, load_new_data\n",
    "import time\n",
    "start_time = time.time()\n",
    "dataf = \"../data/\"\n",
    "norm_type = \"SymNorm_tildeA\"\n",
    "#norm_type = \"sym_normalized_A\"\n",
    "#original_graph, L, features, labels, idx_train,idx_val, idx_test = load_citation_v2(dataf,\"Photo\",norm_type=norm_type,cuda=False, identity_features=False)\n",
    "original_graph, L, features, labels, idx_train,idx_val, idx_test, graph_additional_package, data_package = load_new_data(dataf,\"cora_ml\",cuda=False,norm_type=norm_type,identity_features=False)\n",
    "name = \"Cora-ML\"\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Common Function Definition, including Calculating SPD and KFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import time\n",
    "import pickle as pkl\n",
    "import os\n",
    "def constructNeighborSet(graph_in):\n",
    "    neighbor_set = {}\n",
    "    senders = graph_in[\"senders\"]\n",
    "    receivers = graph_in[\"receivers\"]\n",
    "    for i in range(graph_in[\"node_num\"]):\n",
    "        neighbor_set[i] = []\n",
    "    for i in range(len(senders)):\n",
    "        send_node = senders[i]\n",
    "        if not send_node in neighbor_set:\n",
    "            neighbor_set[send_node] = []\n",
    "        neighbor_set[send_node].append(receivers[i])\n",
    "    return neighbor_set\n",
    "\n",
    "def normalized(dist):\n",
    "    total = sum(dist)\n",
    "    if total == 0:\n",
    "        return dist\n",
    "    else:\n",
    "        dist = [value / total for value in dist]\n",
    "        return dist\n",
    "def getShortestPathDistanceNodes(node_num, neighbor_set, anchor_list, labels):\n",
    "    shortest_path_list = []\n",
    "    anchor_set = set(anchor_list)\n",
    "    num_class = max(labels)+1\n",
    "    #print(anchor_set)\n",
    "    for i in range(node_num):\n",
    "        de = collections.deque([[i,0]])\n",
    "        shortest_path_distance = \"inf\"\n",
    "        shortest_path_train_nodes = []\n",
    "        mask = [False for i in range(node_num)]\n",
    "        class_distribution = [0 for i in range(num_class)]\n",
    "        while len(de)>0:\n",
    "            curr = de.popleft()\n",
    "            mask[curr[0]] = True\n",
    "            if curr[0] in anchor_set:\n",
    "                if shortest_path_distance == \"inf\":\n",
    "                    #shortest_path_train_nodes = [curr[0]]\n",
    "                    class_distribution[labels[curr[0]]] = class_distribution[labels[curr[0]]] + 1\n",
    "                    shortest_path_distance = curr[1]\n",
    "                elif curr[1] == shortest_path_distance:\n",
    "                    class_distribution[labels[curr[0]]] = class_distribution[labels[curr[0]]] + 1\n",
    "                    #shortest_path_train_nodes.append(curr[0])\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                #if curr[1]+1<=2:\n",
    "                neighbors = neighbor_set[curr[0]]\n",
    "                for j in neighbors:\n",
    "                    if not mask[j]:\n",
    "                        de.append([j, curr[1]+1])\n",
    "        shortest_path_list.append({\n",
    "            \"dis\":shortest_path_distance,\n",
    "            \"train_nodes\":normalized(class_distribution)\n",
    "        })    \n",
    "    return shortest_path_list\n",
    "def getShortestPathDistance(original_graph, idx_train, labels):\n",
    "    graph_in = {\n",
    "        \"node_num\":original_graph.size()[0],\n",
    "        \"senders\":original_graph._indices()[1].tolist(),\n",
    "        \"receivers\":original_graph._indices()[0].tolist()\n",
    "    }\n",
    "    node_num = graph_in[\"node_num\"]\n",
    "    cora_gcn_neighbor_set = constructNeighborSet(graph_in)\n",
    "    shortest_path_list = getShortestPathDistanceNodes(node_num, cora_gcn_neighbor_set, idx_train.tolist(), labels.tolist())\n",
    "    return shortest_path_list\n",
    "def getSavePath(dataf, data_name):\n",
    "    return dataf+\"{}/{}_SPD.pkl\".format(data_name, data_name)\n",
    "def saveJson(obj, savePath):\n",
    "    with open(savePath, \"wb\") as f:\n",
    "        pkl.dump(obj, f)\n",
    "def loadJson(savePath):\n",
    "    with open(savePath, \"rb\") as f:\n",
    "        obj = pkl.load(f)\n",
    "    return obj\n",
    "def getSPDJson(original_graph, idx_train, labels, dataf, data_name):\n",
    "    savePath = getSavePath(dataf, data_name)\n",
    "    existence = os.path.isfile(savePath)\n",
    "    if existence:\n",
    "        return loadJson(savePath)\n",
    "    else:\n",
    "        SPD = getShortestPathDistance(original_graph, idx_train, labels)\n",
    "        saveJson(SPD, savePath)\n",
    "        return SPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPD = getSPDJson(original_graph, idx_train, labels, dataf, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def calculateJaccardDistance(a,b):\n",
    "    intersection = a*b\n",
    "    union = 1-(1-a)*(1-b)\n",
    "    inter_sum = intersection.sum()\n",
    "    union_sum = union.sum()\n",
    "    print(a.tolist(),b.tolist())\n",
    "    #print(intersection.tolist(), union.tolist())\n",
    "    return inter_sum / union_sum\n",
    "def calculateCosDistance(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "    a_n = a.norm()\n",
    "    b_n = b.norm()\n",
    "    a_norm = a/ torch.max(a_n, eps * torch.ones(1))\n",
    "    b_norm = b/ torch.max(b_n, eps * torch.ones(1))\n",
    "    sim_mt_1 = a_norm * b_norm\n",
    "    sim_mt = sim_mt_1.sum()\n",
    "    return sim_mt\n",
    "def getTopkFeatureSimilaritySet(features, anchor_list, labels, k=5):\n",
    "    node_num = features.shape[0]\n",
    "    feature_sim_set = []\n",
    "    num_class = max(labels)+1\n",
    "    for i in range(node_num):\n",
    "        feature_similarity_list = []\n",
    "        \n",
    "        for anchor in anchor_list:\n",
    "            jd = calculateCosDistance(features[i], features[anchor])\n",
    "            feature_similarity_list.append({\n",
    "                \"anchor_id\": anchor,\n",
    "                \"anchor_label\": labels[anchor],\n",
    "                \"anchor_similarity\": jd.item()\n",
    "            })\n",
    "        feature_similarity_list = sorted(feature_similarity_list, key=lambda ele: ele[\"anchor_similarity\"], reverse=True)\n",
    "        #print(feature_similarity_list)\n",
    "        #break\n",
    "        feature_similarity_list = feature_similarity_list[:k]\n",
    "        class_distribution = [0 for i in range(num_class)]\n",
    "        for item in feature_similarity_list:\n",
    "            label = item[\"anchor_label\"]\n",
    "            class_distribution[label] = class_distribution[label] + 1\n",
    "        feature_sim_set.append({\n",
    "            \"train_nodes\":normalized(class_distribution),\n",
    "            \"details\":feature_similarity_list\n",
    "        })\n",
    "        #print(feature_sim_set)\n",
    "    return feature_sim_set\n",
    "def getKFSSavePath(dataf, data_name):\n",
    "    return dataf+\"{}/{}_KFS.pkl\".format(data_name, data_name)\n",
    "def getKFSJson(features, idx_train, labels, dataf, data_name, k=5):\n",
    "    savePath = getKFSSavePath(dataf, data_name)\n",
    "    #existence = os.path.isfile(savePath)\n",
    "    #if existence:\n",
    "    #    return loadJson(savePath)\n",
    "    #else:\n",
    "    KFS = getTopkFeatureSimilaritySet(features, idx_train.tolist(), labels.tolist(), k)\n",
    "    #    saveJson(KFS, savePath)\n",
    "    return KFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFS = getKFSJson(features, idx_train, labels, dataf, name, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_nodes': [0.0, 0.6, 0.0, 0.0, 0.4, 0.0, 0.0], 'details': [{'anchor_id': 1151, 'anchor_label': 1, 'anchor_similarity': 0.1616433560848236}, {'anchor_id': 1066, 'anchor_label': 1, 'anchor_similarity': 0.1566666215658188}, {'anchor_id': 874, 'anchor_label': 4, 'anchor_similarity': 0.14626014232635498}, {'anchor_id': 2238, 'anchor_label': 4, 'anchor_similarity': 0.13114015758037567}, {'anchor_id': 2419, 'anchor_label': 1, 'anchor_similarity': 0.12761421501636505}]}\n"
     ]
    }
   ],
   "source": [
    "print(KFS[1211])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_nodes': [0.0, 0.6, 0.0, 0.0, 0.2, 0.0, 0.2], 'details': [{'anchor_id': 1151, 'anchor_label': 1, 'anchor_similarity': 0.14745144546031952}, {'anchor_id': 1066, 'anchor_label': 1, 'anchor_similarity': 0.1429116427898407}, {'anchor_id': 1490, 'anchor_label': 6, 'anchor_similarity': 0.1344635933637619}, {'anchor_id': 874, 'anchor_label': 4, 'anchor_similarity': 0.13341885805130005}, {'anchor_id': 2419, 'anchor_label': 1, 'anchor_similarity': 0.12534424662590027}]}\n"
     ]
    }
   ],
   "source": [
    "print(KFS[1210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------  Backup Code -------------------------------  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import time\n",
    "start_time = time.time()\n",
    "def getShortestPathDistance(node_num, neighbor_set, anchor_list):\n",
    "    shortest_path_list = []\n",
    "    anchor_set = set(anchor_list)\n",
    "    #print(anchor_set)\n",
    "    for i in range(node_num):\n",
    "        de = collections.deque([[i,0]])\n",
    "        shortest_path_distance = \"inf\"\n",
    "        mask = [False for i in range(node_num)]\n",
    "        while len(de)>0:\n",
    "            curr = de.popleft()\n",
    "            mask[curr[0]] = True\n",
    "            if curr[0] in anchor_set:\n",
    "                shortest_path_distance = curr[1]\n",
    "                break\n",
    "            else:\n",
    "                #if curr[1]+1<=2:\n",
    "                neighbors = neighbor_set[curr[0]]\n",
    "                for j in neighbors:\n",
    "                    if not mask[j]:\n",
    "                        de.append([j, curr[1]+1])\n",
    "        shortest_path_list.append(shortest_path_distance)    \n",
    "    return shortest_path_list\n",
    "node_num = graph_in[\"node_num\"]\n",
    "shortest_path_list = getShortestPathDistance(node_num, cora_gcn_neighbor_set, idx_train.tolist())\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import time\n",
    "start_time = time.time()\n",
    "def normalized(dist):\n",
    "    total = sum(dist)\n",
    "    if total == 0:\n",
    "        return dist\n",
    "    else:\n",
    "        dist = [value / total for value in dist]\n",
    "        return dist\n",
    "def getShortestPathDistanceNodes(node_num, neighbor_set, anchor_list, labels):\n",
    "    shortest_path_list = []\n",
    "    #anchor_set = set(anchor_list)\n",
    "    num_class = max(labels)+1\n",
    "    #print(anchor_set)\n",
    "    for i in range(node_num):\n",
    "        shortest_path_list.append({\n",
    "            \"shortest_path_distance\":\"inf\",\n",
    "            \"shortest_path_train_nodes\":[]\n",
    "        })\n",
    "    for anchor in anchor_list:\n",
    "        de = collections.deque([[anchor,0]])\n",
    "        #shortest_path_distance = \"inf\"\n",
    "        #shortest_path_train_nodes = []\n",
    "        mask = [False for i in range(node_num)]\n",
    "        #class_distribution = [0 for i in range(num_class)]\n",
    "        while len(de)>0:\n",
    "            curr = de.popleft()\n",
    "            mask[curr[0]] = True\n",
    "            sp = shortest_path_list[curr[0]]\n",
    "            sp_distance = sp[\"shortest_path_distance\"]\n",
    "            if sp_distance == \"inf\" or sp_distance > curr[1]:\n",
    "                shortest_path_list[curr[0]][\"shortest_path_distance\"]=curr[1]\n",
    "                shortest_path_list[curr[0]][\"shortest_path_train_nodes\"]=[anchor]\n",
    "                #shortest_path_train_nodes = [curr[0]]\n",
    "                #class_distribution[labels[curr[0]]] = class_distribution[labels[curr[0]]] + 1\n",
    "                #shortest_path_distance = curr[1]\n",
    "            elif curr[1] == sp_distance:\n",
    "                shortest_path_list[curr[0]][\"shortest_path_train_nodes\"].append(anchor)\n",
    "                #class_distribution[labels[curr[0]]] = class_distribution[labels[curr[0]]] + 1\n",
    "                #shortest_path_train_nodes.append(curr[0])\n",
    "\n",
    "            neighbors = neighbor_set[curr[0]]\n",
    "            for j in neighbors:\n",
    "                if not mask[j]:\n",
    "                    de.append([j, curr[1]+1])\n",
    "        \n",
    "    return shortest_path_list\n",
    "node_num = graph_in[\"node_num\"]\n",
    "shortest_path_list = getShortestPathDistanceNodes(node_num, cora_gcn_neighbor_set, idx_train.tolist(), labels.tolist())\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_new_data\n",
    "import time\n",
    "start_time = time.time()\n",
    "dataf = \"../data/\"\n",
    "norm_type = \"SymNorm_tildeA\"\n",
    "#norm_type = \"sym_normalized_A\"\n",
    "original_graph, L, features, labels, idx_train,idx_val, idx_test, graph_info = load_new_data(dataf,\"polblogs\",norm_type=norm_type,cuda=False, identity_features=True)\n",
    "name = \"polblogs\"\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/github/GNNVis/server/data/g2g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from models import GCN\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import accuracy, set_seed\n",
    "\n",
    "\n",
    "model_path = \"../models/gcn_photo_state.pkt\"\n",
    "#model_path = \"../models/mlp_cora_state.pkt\"\n",
    "#torch.save(model.state_dict(), model_path)\n",
    "#args = [1433,16,7,0.5]\n",
    "#args = [3703,16,6,0.5]\n",
    "#args = [500, 16, 3, 0.5]\n",
    "#args = [1222, 16, 2, 0.5]\n",
    "args = [745, 16, 8, 0.5]\n",
    "kwargs = {\n",
    "    \"bias\": True,\n",
    "}\n",
    "model = GCN(*args,**kwargs)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "model.eval()\n",
    "output = model(features,L)\n",
    "loss_test = F.nll_loss(output[idx_test],labels[idx_test])\n",
    "acc_test = accuracy(output[idx_test],labels[idx_test])\n",
    "print(loss_test.item(), acc_test.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.requires_grad_()\n",
    "output = model(features,L)\n",
    "N = 645\n",
    "node_relevance = torch.zeros_like(output)\n",
    "node_relevance[N] = 1\n",
    "output.backward(node_relevance)\n",
    "\n",
    "node_feature_importance = features.grad\n",
    "node_importance = features.grad.pow(2).sum(dim=1)\n",
    "node_importance = node_importance.tolist()\n",
    "\n",
    "#for i in range(len(node_importance)):\n",
    "#    if not node_importance[i] == 0:\n",
    "#        pass\n",
    "#        #print(i, node_importance[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance = features.grad[645]\n",
    "features_importance = features_importance.tolist()\n",
    "feature_index = sorted(range(len(features_importance)), key=lambda k: -features_importance[k])\n",
    "for index in feature_index:\n",
    "    print(index, features_importance[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_out = net(batch)[0]\n",
    "\n",
    "N = node_no\n",
    "node_relevance = torch.zeros_like(graph_out.node_features)\n",
    "node_relevance[N] = 1\n",
    "\n",
    "graph_in.zero_grad_()\n",
    "graph_out.node_features.backward(node_relevance)\n",
    "\n",
    "node_importance = batch.node_features.grad.pow(2).sum(dim=1)\n",
    "edge_importance = batch.edge_features.grad.pow(2).sum(dim=1)\n",
    "return node_importance, edge_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()[\"gc1.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; sns.set()\n",
    "#uniform_data = np.random.rand(10, 12)\n",
    "ax = sns.heatmap(model.state_dict()[\"gc2.weight\"].cpu().numpy())\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"gc2_weight.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_graph._indices()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from fa2 import ForceAtlas2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.Graph()\n",
    "indices = original_graph._indices().tolist()\n",
    "edge_num = len(indices[0])\n",
    "edge_index = [(indices[0][i], indices[1][i]) for i in range(edge_num)]\n",
    "node_index = list(range(features.shape[0]))\n",
    "G.add_nodes_from(node_index)\n",
    "G.add_edges_from(edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=2.0,\n",
    "                        strongGravityMode=False,\n",
    "                        gravity=1.0,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(G, pos=None, iterations=500)\n",
    "nx.draw_networkx_nodes(G, positions, node_size=20, with_labels=False, node_color=\"blue\", alpha=0.4)\n",
    "nx.draw_networkx_edges(G, positions, edge_color=\"green\", alpha=0.05)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# equivalently\n",
    "#import igraph\n",
    "#G = igraph.Graph.TupleList(G.edges(), directed=False)\n",
    "#layout = forceatlas2.forceatlas2_igraph_layout(G, pos=None, iterations=2000)\n",
    "#igraph.plot(G, layout).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist = list(positions.keys())\n",
    "newlist.sort()\n",
    "newPos = []\n",
    "for i in range(len(newlist)):\n",
    "    newPos.append([positions[i][0], positions[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"../data/{}/{}_layout.pkt\".format(name,name),\"wb\") as f:\n",
    "    pkl.dump(newPos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"../data/{}/{}_layout.pkt\".format(name,name),\"rb\") as f:\n",
    "    newPos = pkl.load(f)\n",
    "    print(newPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import skcuda.linalg as sklin\n",
    "from layer import GCN_layer\n",
    "from utils import accuracy\n",
    "'''\n",
    "GCN_layer(ind,outd,bias=True)\n",
    "'''\n",
    "\n",
    "class GCN_hook(nn.Module):\n",
    "    def __init__(self, num_feature,num_hidden,num_class,dropout,bias=True):\n",
    "        super(GCN_hook,self).__init__()\n",
    "\n",
    "        self.gc1 = GCN_layer(num_feature, num_hidden)\n",
    "        self.gc2 = GCN_layer(num_hidden, num_class)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x1 = F.dropout(x, self.dropout, training=self.training)\n",
    "        x2 = self.gc2(x1, adj)\n",
    "        return F.log_softmax(x2, dim=1), x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2 = \"photo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/gcn_{}_state.pkt\".format(name2)\n",
    "#torch.save(model.state_dict(), model_path)\n",
    "#args_input = [3703,16,6,0.5]\n",
    "#args_input = [500, 16, 3, 0.5]\n",
    "#args_input = [2879, 16, 7, 0.5]\n",
    "#args_input = [1222, 16, 2, 0.5]\n",
    "args_input = [745, 16, 8, 0.5]\n",
    "kwargs = {\n",
    "    \"bias\": True\n",
    "}\n",
    "model = GCN_hook(*args_input,**kwargs)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "model.eval()\n",
    "output, inner_state = model(features,L)\n",
    "loss_test = F.nll_loss(output[idx_test],labels[idx_test])\n",
    "acc_test = accuracy(output[idx_test],labels[idx_test])\n",
    "print(loss_test.item(), acc_test.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "def dimension_reduction(input_array):\n",
    "    start_time = time.time()\n",
    "    X = np.array(input_array)    \n",
    "    X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "    print(time.time() - start_time)\n",
    "    return X_embedded\n",
    "def visualize(embedded_array,labels):\n",
    "    \n",
    "    #sns.set_palette(sns.color_palette(\"Paired\"))\n",
    "    X = np.array(embedded_array)\n",
    "    labels = np.array(labels)\n",
    "    labels = np.expand_dims(labels, axis=1)\n",
    "    data = np.concatenate((X, labels), axis=1)\n",
    "    df = pd.DataFrame(data, columns=[\"x\", \"y\",\"Labels\"])\n",
    "    # Create an array with the colors you want to use\n",
    "    colors = [\"#FF0B04\", \"#4374B3\"]\n",
    "    # Set your custom color palette\n",
    "    customPalette = sns.set_palette(sns.color_palette(colors))\n",
    "    ax = sns.scatterplot(x=\"x\", y=\"y\",hue=\"Labels\", data=df, palette=\"Set1\", legend=False)\n",
    "    \n",
    "current_palette = sns.color_palette()\n",
    "sns.palplot(current_palette)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input Layer\n",
    "features_array = features.cpu().detach().numpy()\n",
    "features_embeddded_array = dimension_reduction(features_array)\n",
    "visualize(features_embeddded_array, labels.cpu().detach().numpy())\n",
    "with open(\"../data/{}/{}_tsne_input.pkt\".format(name,name),\"wb\") as f:\n",
    "    pkl.dump(features_embeddded_array, f)\n",
    "    print(\"Done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hidden Layer\n",
    "labels_array = labels.cpu().detach().numpy()\n",
    "layer2 = inner_state\n",
    "layer2_array = layer2.cpu().detach().numpy()\n",
    "layer2_embeddded_array = dimension_reduction(layer2_array)\n",
    "visualize(layer2_embeddded_array, labels.cpu().detach().numpy())\n",
    "with open(\"../data/{}/{}_tsne_hidden.pkt\".format(name,name),\"wb\") as f:\n",
    "    pkl.dump(layer2_embeddded_array, f)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output Layer\n",
    "input_array = output.cpu().detach().numpy()\n",
    "labels_array = labels.cpu().detach().numpy()\n",
    "embedded_array = dimension_reduction(input_array)\n",
    "visualize(embedded_array, labels.cpu().detach().numpy())\n",
    "with open(\"../data/{}/{}_tsne_output.pkt\".format(name,name),\"wb\") as f:\n",
    "    pkl.dump(embedded_array, f)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
