{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_citation\n",
    "import time\n",
    "start_time = time.time()\n",
    "dataf = \"../data/\"\n",
    "norm_type = \"SymNorm_tildeA\"\n",
    "#norm_type = \"sym_normalized_A\"\n",
    "original_graph, L, features, labels, idx_train,idx_val, idx_test = load_citation(dataf,\"cora\",norm_type=norm_type,cuda=False, identity_features=False)\n",
    "name = \"cora\"\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from models import GCN\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import accuracy, set_seed\n",
    "\n",
    "\n",
    "model_path = \"../models/gcn_cora_state.pkt\"\n",
    "#model_path = \"../models/mlp_cora_state.pkt\"\n",
    "#torch.save(model.state_dict(), model_path)\n",
    "args = [1433,16,7,0.5]\n",
    "#args = [3703,16,6,0.5]\n",
    "#args = [500, 16, 3, 0.5]\n",
    "#args = [1222, 16, 2, 0.5]\n",
    "kwargs = {\n",
    "    \"bias\": True,\n",
    "}\n",
    "model = GCN(*args,**kwargs)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "model.eval()\n",
    "output = model(features,L)\n",
    "loss_test = F.nll_loss(output[idx_test],labels[idx_test])\n",
    "acc_test = accuracy(output[idx_test],labels[idx_test])\n",
    "print(loss_test.item(), acc_test.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "import models\n",
    "#import utils.io_utils as io_utils\n",
    "#import utils.parser_utils as parser_utils\n",
    "#import utils.graph_utils as graph_utils\n",
    "from explainer import explain\n",
    "\"\"\" parser_utils.py\n",
    "\n",
    "    Parsing utilities.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "def parse_optimizer(parser):\n",
    "    '''Set optimizer parameters'''\n",
    "    opt_parser = parser.add_argument_group()\n",
    "    opt_parser.add_argument('--opt', dest='opt', type=str,\n",
    "            help='Type of optimizer')\n",
    "    opt_parser.add_argument('--opt-scheduler', dest='opt_scheduler', type=str,\n",
    "            help='Type of optimizer scheduler. By default none')\n",
    "    opt_parser.add_argument('--opt-restart', dest='opt_restart', type=int,\n",
    "            help='Number of epochs before restart (by default set to 0 which means no restart)')\n",
    "    opt_parser.add_argument('--opt-decay-step', dest='opt_decay_step', type=int,\n",
    "            help='Number of epochs before decay')\n",
    "    opt_parser.add_argument('--opt-decay-rate', dest='opt_decay_rate', type=float,\n",
    "            help='Learning rate decay ratio')\n",
    "    opt_parser.add_argument('--lr', dest='lr', type=float,\n",
    "            help='Learning rate.')\n",
    "    opt_parser.add_argument('--clip', dest='clip', type=float,\n",
    "            help='Gradient clipping.')\n",
    "\n",
    "\n",
    "def arg_parse():\n",
    "    parser = argparse.ArgumentParser(description=\"GNN Explainer arguments.\")\n",
    "    io_parser = parser.add_mutually_exclusive_group(required=False)\n",
    "    io_parser.add_argument(\"--dataset\", dest=\"dataset\", help=\"Input dataset.\")\n",
    "    benchmark_parser = io_parser.add_argument_group()\n",
    "    benchmark_parser.add_argument(\n",
    "        \"--bmname\", dest=\"bmname\", help=\"Name of the benchmark dataset\"\n",
    "    )\n",
    "    io_parser.add_argument(\"--pkl\", dest=\"pkl_fname\", help=\"Name of the pkl data file\")\n",
    "\n",
    "    parse_optimizer(parser)\n",
    "\n",
    "    parser.add_argument(\"--clean-log\", action=\"store_true\", help=\"If true, cleans the specified log directory before running.\")\n",
    "    parser.add_argument(\"--logdir\", dest=\"logdir\", help=\"Tensorboard log directory\")\n",
    "    parser.add_argument(\"--ckptdir\", dest=\"ckptdir\", help=\"Model checkpoint directory\")\n",
    "    parser.add_argument(\"--cuda\", dest=\"cuda\", help=\"CUDA.\")\n",
    "    parser.add_argument(\n",
    "        \"--gpu\",\n",
    "        dest=\"gpu\",\n",
    "        action=\"store_const\",\n",
    "        const=True,\n",
    "        default=False,\n",
    "        help=\"whether to use GPU.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", dest=\"num_epochs\", type=int, help=\"Number of epochs to train.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden-dim\", dest=\"hidden_dim\", type=int, help=\"Hidden dimension\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dim\", dest=\"output_dim\", type=int, help=\"Output dimension\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-gc-layers\",\n",
    "        dest=\"num_gc_layers\",\n",
    "        type=int,\n",
    "        help=\"Number of graph convolution layers before each pooling\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--bn\",\n",
    "        dest=\"bn\",\n",
    "        action=\"store_const\",\n",
    "        const=True,\n",
    "        default=False,\n",
    "        help=\"Whether batch normalization is used\",\n",
    "    )\n",
    "    parser.add_argument(\"--dropout\", dest=\"dropout\", type=float, help=\"Dropout rate.\")\n",
    "    parser.add_argument(\n",
    "        \"--nobias\",\n",
    "        dest=\"bias\",\n",
    "        action=\"store_const\",\n",
    "        const=False,\n",
    "        default=True,\n",
    "        help=\"Whether to add bias. Default to True.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-writer\",\n",
    "        dest=\"writer\",\n",
    "        action=\"store_const\",\n",
    "        const=False,\n",
    "        default=False,\n",
    "        help=\"Whether to add bias. Default to True.\",\n",
    "    )\n",
    "    # Explainer\n",
    "    parser.add_argument(\"--mask-act\", dest=\"mask_act\", type=str, help=\"sigmoid, ReLU.\")\n",
    "    parser.add_argument(\n",
    "        \"--mask-bias\",\n",
    "        dest=\"mask_bias\",\n",
    "        action=\"store_const\",\n",
    "        const=True,\n",
    "        default=False,\n",
    "        help=\"Whether to add bias. Default to True.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--explain-node\", dest=\"explain_node\", type=int, help=\"Node to explain.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--graph-idx\", dest=\"graph_idx\", type=int, help=\"Graph to explain.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--graph-mode\",\n",
    "        dest=\"graph_mode\",\n",
    "        action=\"store_const\",\n",
    "        const=True,\n",
    "        default=False,\n",
    "        help=\"whether to run Explainer on Graph Classification task.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--multigraph-class\",\n",
    "        dest=\"multigraph_class\",\n",
    "        type=int,\n",
    "        help=\"whether to run Explainer on multiple Graphs from the Classification task for examples in the same class.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--multinode-class\",\n",
    "        dest=\"multinode_class\",\n",
    "        type=int,\n",
    "        help=\"whether to run Explainer on multiple nodes from the Classification task for examples in the same class.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--align-steps\",\n",
    "        dest=\"align_steps\",\n",
    "        type=int,\n",
    "        help=\"Number of iterations to find P, the alignment matrix.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--method\", dest=\"method\", type=str, help=\"Method. Possible values: base, att.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--name-suffix\", dest=\"name_suffix\", help=\"suffix added to the output filename\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--explainer-suffix\",\n",
    "        dest=\"explainer_suffix\",\n",
    "        help=\"suffix added to the explainer log\",\n",
    "    )\n",
    "\n",
    "    # TODO: Check argument usage\n",
    "    parser.set_defaults(\n",
    "        logdir=\"log\",\n",
    "        ckptdir=\"ckpt\",\n",
    "        dataset=\"syn1\",\n",
    "        opt=\"adam\",  \n",
    "        opt_scheduler=\"none\",\n",
    "        cuda=\"0\",\n",
    "        lr=0.1,\n",
    "        clip=2.0,\n",
    "        batch_size=20,\n",
    "        num_epochs=100,\n",
    "        hidden_dim=20,\n",
    "        output_dim=20,\n",
    "        num_gc_layers=2,\n",
    "        dropout=0.0,\n",
    "        method=\"base\",\n",
    "        name_suffix=\"\",\n",
    "        explainer_suffix=\"\",\n",
    "        align_steps=1000,\n",
    "        explain_node=None,\n",
    "        graph_idx=-1,\n",
    "        mask_act=\"sigmoid\",\n",
    "        multigraph_class=-1,\n",
    "        multinode_class=-1,\n",
    "    )\n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a configuration\n",
    "prog_args = arg_parse()\n",
    "cg_dict = {}\n",
    "cg_dict[\"adj\"] = original_graph.to_dense().unsqueeze(0).detach().numpy()\n",
    "cg_dict[\"feat\"] = features.unsqueeze(0).detach().numpy()\n",
    "cg_dict[\"label\"] = labels.unsqueeze(0).detach().numpy()\n",
    "cg_dict[\"train_idx\"] = idx_train.unsqueeze(0).detach().numpy()\n",
    "cg_dict[\"pred\"] = output.unsqueeze(0).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cg_dict[\"adj\"].shape, cg_dict[\"feat\"].shape, cg_dict[\"label\"].shape, cg_dict[\"train_idx\"].shape, cg_dict[\"pred\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create explainer\n",
    "explainer = explain.Explainer(\n",
    "    model=model,\n",
    "    adj=cg_dict[\"adj\"],\n",
    "    feat=cg_dict[\"feat\"],\n",
    "    label=cg_dict[\"label\"],\n",
    "    pred=cg_dict[\"pred\"],\n",
    "    train_idx=cg_dict[\"train_idx\"],\n",
    "    args=prog_args,\n",
    "    writer=None,\n",
    "    print_training=False,\n",
    "    graph_mode=False,\n",
    "    graph_idx=0,\n",
    ")\n",
    "print(prog_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "adj = cg_dict[\"adj\"]\n",
    "node_idx_new, sub_adj, sub_feat, sub_label, neighbors = explainer.extract_neighborhood(explain_node_idx)\n",
    "neighbors_dict = {}\n",
    "for i in range(len(neighbors)):\n",
    "    neighbors_dict[i] = neighbors[i]\n",
    "def constructNewNeighborDict(node_list, neighbor_dict):\n",
    "    new_neighbor_dict = {}\n",
    "    for node_id in node_list:\n",
    "        new_neighbor_dict[node_id] = neighbor_dict[node_id]\n",
    "    return new_neighbor_dict\n",
    "for m in masked_adj: \n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "    #plt.title(str(m))\n",
    "    \n",
    "    # Full adjacency\n",
    "    ax1.set_title('Full Graph')\n",
    "    #ax1.imshow(adj)\n",
    "    #G1 = nx.from_numpy_array(adj[0])\n",
    "    #G1.remove_nodes_from(list(nx.isolates(G1)))\n",
    "    #nx.draw(G, ax=ax3)\n",
    "    #nx.draw(G1, ax=ax1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Filtered adjacency\n",
    "    f = filter_adj(m,0.3)\n",
    "    #ax2.set_title('Sub neighbor')\n",
    "    #ax2.imshow(f);\n",
    "    \n",
    "    G2 = nx.from_numpy_array(sub_adj)\n",
    "    G2.remove_nodes_from(list(nx.isolates(G2)))\n",
    "    nx.draw_networkx(G2, ax=ax2, labels=constructNewNeighborDict(G2.nodes,neighbors_dict))\n",
    "    \n",
    "    # Plot subgraph\n",
    "    ax3.set_title(\"Explanation Sub graph\")\n",
    "    G = nx.from_numpy_array(f)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    \n",
    "    \n",
    "    #nx.draw(G, ax=ax3)\n",
    "    nx.draw_networkx(G, ax=ax3, labels=constructNewNeighborDict(G.nodes,neighbors_dict))\n",
    "    '''\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_node_idx = 1892\n",
    "# explain a set of nodes\n",
    "masked_adj = explainer.explain_nodes_gnn_stats(\n",
    "    [explain_node_idx], prog_args\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "def show_adjacency_full(mask, ax=None):\n",
    "    adj = np.load(os.path.join(logdir, expdir, mask), allow_pickle=True)\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        plt.imshow(adj);\n",
    "    else:\n",
    "        ax.imshow(adj)\n",
    "    return adj\n",
    "def filter_adj(adj,threshold=0.8):\n",
    "    filt_adj = adj.copy()\n",
    "    filt_adj[adj<threshold] = 0\n",
    "    return filt_adj\n",
    "adj = cg_dict[\"adj\"]\n",
    "node_idx_new, sub_adj, sub_feat, sub_label, neighbors = explainer.extract_neighborhood(explain_node_idx)\n",
    "print(neighbors)\n",
    "neighbors_dict = {}\n",
    "for i in range(len(neighbors)):\n",
    "    neighbors_dict[i] = neighbors[i]\n",
    "def constructNewNeighborDict(node_list, neighbor_dict):\n",
    "    new_neighbor_dict = {}\n",
    "    for node_id in node_list:\n",
    "        new_neighbor_dict[node_id] = neighbor_dict[node_id]\n",
    "    return new_neighbor_dict\n",
    "ground_truth_label = cg_dict[\"label\"][0].tolist()\n",
    "preds = cg_dict[\"pred\"][0].argmax(1).tolist()\n",
    "TF = []\n",
    "for i in range(len(ground_truth_label)):\n",
    "    if preds[i] == ground_truth_label[i]:\n",
    "        TF.append(1)\n",
    "    else:\n",
    "        TF.append(0)\n",
    "#print(TF)\n",
    "'''\n",
    "ground_truth_color = []\n",
    "prediction_color = []\n",
    "TF_color = []\n",
    "for i in range(len(ground_truth_label)):\n",
    "    ground_truth_color.append(d3_10color[ground_truth_label[i]])\n",
    "    prediction_color.append(d3_10color[preds[i]])\n",
    "    TF_color.append(tf_color[TF[i]])\n",
    "'''\n",
    "# EDIT THIS INDEX\n",
    "MASK_IDX = 0\n",
    "# EDIT THIS INDEX\n",
    "\n",
    "m = masked_adj[MASK_IDX]\n",
    "d3_10color = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"]\n",
    "tf_color = [\"#DC143C\",\"#498B77\"]\n",
    "def constructColorList(nodelist, labellist, colorlist):\n",
    "    new_colorlist = []\n",
    "    for node_idx in nodelist:\n",
    "        node_id = nodelist[node_idx]\n",
    "        label = labellist[node_id]\n",
    "        new_colorlist.append(colorlist[label])\n",
    "        #if node_id == 1708:\n",
    "        #print(node_id, label, colorlist[label])\n",
    "    return new_colorlist\n",
    "def constructNodeSizeList(nodelist, explain_node_idx):\n",
    "    sizelist = []\n",
    "    for node_idx in nodelist:\n",
    "        node_id = nodelist[node_idx]\n",
    "        if node_id == explain_node_idx:\n",
    "            sizelist.append(500)\n",
    "        else:\n",
    "            sizelist.append(300)\n",
    "    return sizelist\n",
    "@interact(thresh=widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01))\n",
    "def plot_interactive(thresh=0.5):\n",
    "    f = filter_adj(m,thresh)\n",
    "    G = nx.from_numpy_array(f)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    pos = nx.spring_layout(G)\n",
    "    #G2 = nx.from_numpy_array(sub_adj)\n",
    "    #G2.remove_nodes_from(list(nx.isolates(G2)))\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "    plt.title(\"Explanation Subgraph threshold:{}\".format(thresh))\n",
    "    \n",
    "    ax1.set_title('Ground Truth ')\n",
    "    real_label = constructNewNeighborDict(G.nodes,neighbors_dict)\n",
    "    nx.draw_networkx(G, pos=pos, ax=ax1, labels=real_label, \n",
    "                     node_color=constructColorList(real_label,ground_truth_label,d3_10color)\n",
    "                    ,node_size=constructNodeSizeList(real_label,explain_node_idx))\n",
    "\n",
    "    # Filtered adjacency\n",
    "    \n",
    "    ax2.set_title('Model Prediction')\n",
    "    nx.draw_networkx(G, pos=pos,ax=ax2, labels=real_label, \n",
    "                     node_color=constructColorList(real_label,preds,d3_10color)\n",
    "                    ,node_size=constructNodeSizeList(real_label,explain_node_idx))\n",
    "    \n",
    "    # Plot subgraph\n",
    "    ax3.set_title(\"T/F Explain Node Id:{} threshold:{}\".format(explain_node_idx,thresh))\n",
    "    nx.draw_networkx(G, pos=pos, ax=ax3, labels=real_label, \n",
    "                     node_color=constructColorList(real_label,TF,tf_color)\n",
    "                    ,node_size=constructNodeSizeList(real_label,explain_node_idx))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
