{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Args and Import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from models import GCN\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import accuracy, set_seed\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--epochs',type=int,default=200,help='Number of epochs to train')\n",
    "parser.add_argument('--seed',type=int,default=42,help='random seed')\n",
    "parser.add_argument('--lr',type=float,default=0.01,help='Initial learning rate,adam:0.001,sgd:0.05')\n",
    "parser.add_argument('--weight_decay',type=float,default=5e-4,help='weight decay') #5e-4\n",
    "parser.add_argument('--hidden',type=int,default=16,help='Number of hidden units')\n",
    "parser.add_argument('--dropout',type=float,default=0.5,help='Dropout rate')\n",
    "parser.add_argument('--dataset',type=str,default='Cora',help='name of dataset:Cora')\n",
    "parser.add_argument('--optimize_type',type=str,default='Adam', help='Optimization method, Adam; SGD')\n",
    "parser.add_argument('--model_name',type=str,default='GCN',help='name of model:GCN')\n",
    "parser.add_argument('--norm_type',type=str,default='SymNorm_tildeA',help= 'Normalization method:SymNorm_tildeA')\n",
    "parser.add_argument('--layer_num',type=int,default=6,help='layer num for GCN_nlayer')\n",
    "parser.add_argument('--data_path', type=str,default='../data/', help='the path for dataset')\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path='../data/', dataset='Cora', dropout=0.5, epochs=200, hidden=16, layer_num=6, lr=0.01, model_name='GCN', norm_type='SymNorm_tildeA', optimize_type='Adam', seed=42, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_citation, load_citation_v2, load_citation_v3\n",
    "import time\n",
    "start_time = time.time()\n",
    "dataf = args.data_path\n",
    "norm_type = args.norm_type\n",
    "#norm_type = \"sym_normalized_A\"\n",
    "original_graph, L, features, labels, idx_train,idx_val, idx_test, data_package = load_citation_v3(dataf,\n",
    "                                                                                                  args.dataset,\n",
    "                                                                                                  norm_type=norm_type,\n",
    "                                                                                                  cuda=True, \n",
    "                                                                                                  identity_features=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_node: 2708\n",
      "num_feature:  1433\n",
      "num_class 7\n"
     ]
    }
   ],
   "source": [
    "num_node = features.shape[0]\n",
    "print('num_node:',features.shape[0])\n",
    "num_feature = features.shape[1]  \n",
    "print('num_feature: ',features.shape[1])\n",
    "num_class = labels.max().item()+1\n",
    "print('num_class',num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708]) torch.Size([2708]) torch.Size([2708])\n"
     ]
    }
   ],
   "source": [
    "split_train = [i for i in range(num_node)]\n",
    "idx_train = torch.LongTensor(split_train).cuda()\n",
    "idx_test = torch.LongTensor(split_train).cuda()\n",
    "idx_valid = torch.LongTensor(split_train).cuda()\n",
    "print(idx_train.shape, idx_test.shape, idx_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model and Optimizer Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433 16 7 0.5 6 True\n",
      "model:GCN, Norm:SymNorm_tildeA, optimizer: Adam, hidden:16\n",
      "GCN(\n",
      "  (gc1): GCN_layer (1433 -> 16)\n",
      "  (gc2): GCN_layer (16 -> 7)\n",
      ")\n",
      "gc1.weight : 1433x16 = 22928\n",
      "gc1.bias : 16\n",
      "gc2.weight : 16x7 = 112\n",
      "gc2.bias : 7\n",
      "23063\n"
     ]
    }
   ],
   "source": [
    "# 网络加深，反而效果差了\n",
    "print(num_feature, args.hidden, num_class, args.dropout, args.layer_num, True)\n",
    "if args.model_name == 'GCN':\n",
    "    model = GCN( num_feature,args.hidden,num_class,args.dropout,bias=True).cuda()\n",
    "else:\n",
    "    raise Exception('Invalid model:',args.model_name)\n",
    "if args.optimize_type == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(),lr=args.lr,weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(),lr=args.lr,weight_decay=args.weight_decay)\n",
    "print('model:%s, Norm:%s, optimizer: %s, hidden:%d'% (args.model_name, args.norm_type,args.optimize_type,args.hidden))\n",
    "print(model)\n",
    "def count_parameters(model):\n",
    "    total_param = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_param = np.prod(param.size())\n",
    "            if param.dim() > 1:\n",
    "                print(name, ':', 'x'.join(str(x) for x in list(param.size())), '=', num_param)\n",
    "            else:\n",
    "                print(name, ':', num_param)\n",
    "            total_param += num_param\n",
    "    return total_param\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training----\n",
      "0\n",
      "Epoch : 0, Time : 0.5862569808959961, Val_loss: 1.8997282981872559, Val_Acc: 0.24519940915805025\n",
      "Epoch : 1, Time : 0.010206937789916992, Val_loss: 1.8241863250732422, Val_Acc: 0.33345642540620385\n",
      "Epoch : 2, Time : 0.008930206298828125, Val_loss: 1.7573879957199097, Val_Acc: 0.3393648449039882\n",
      "Epoch : 3, Time : 0.008974313735961914, Val_loss: 1.6930315494537354, Val_Acc: 0.33825701624815363\n",
      "Epoch : 4, Time : 0.009635448455810547, Val_loss: 1.6287351846694946, Val_Acc: 0.35081240768094535\n",
      "Epoch : 5, Time : 0.009485244750976562, Val_loss: 1.5637441873550415, Val_Acc: 0.37629246676514033\n",
      "Epoch : 6, Time : 0.009327411651611328, Val_loss: 1.4975639581680298, Val_Acc: 0.4224519940915805\n",
      "Epoch : 7, Time : 0.009190559387207031, Val_loss: 1.4303228855133057, Val_Acc: 0.4974150664697194\n",
      "Epoch : 8, Time : 0.00877523422241211, Val_loss: 1.3637577295303345, Val_Acc: 0.5605612998522895\n",
      "Epoch : 9, Time : 0.009434700012207031, Val_loss: 1.2988815307617188, Val_Acc: 0.6096750369276219\n",
      "Epoch : 10, Time : 0.009960174560546875, Val_loss: 1.2361619472503662, Val_Acc: 0.6558345642540621\n",
      "Epoch : 11, Time : 0.009610891342163086, Val_loss: 1.1752433776855469, Val_Acc: 0.6794682422451994\n",
      "Epoch : 12, Time : 0.00911259651184082, Val_loss: 1.115154504776001, Val_Acc: 0.7163958641063516\n",
      "Epoch : 13, Time : 0.009360074996948242, Val_loss: 1.0551341772079468, Val_Acc: 0.7433530280649927\n",
      "Epoch : 14, Time : 0.009532690048217773, Val_loss: 0.9957519769668579, Val_Acc: 0.7695716395864107\n",
      "Epoch : 15, Time : 0.009446859359741211, Val_loss: 0.9367855191230774, Val_Acc: 0.7965288035450517\n",
      "Epoch : 16, Time : 0.010025978088378906, Val_loss: 0.8790920376777649, Val_Acc: 0.8223781388478583\n",
      "Epoch : 17, Time : 0.010358810424804688, Val_loss: 0.8231520056724548, Val_Acc: 0.8419497784342689\n",
      "Epoch : 18, Time : 0.009914398193359375, Val_loss: 0.7695110440254211, Val_Acc: 0.8596750369276219\n",
      "Epoch : 19, Time : 0.010167598724365234, Val_loss: 0.7183796763420105, Val_Acc: 0.8703840472673561\n",
      "Epoch : 20, Time : 0.010459661483764648, Val_loss: 0.6702880859375, Val_Acc: 0.878138847858198\n",
      "Epoch : 21, Time : 0.011174917221069336, Val_loss: 0.6253721714019775, Val_Acc: 0.8851550960118169\n",
      "Epoch : 22, Time : 0.010540485382080078, Val_loss: 0.5839753746986389, Val_Acc: 0.8884785819793206\n",
      "Epoch : 23, Time : 0.009953022003173828, Val_loss: 0.5461409091949463, Val_Acc: 0.8929098966026588\n",
      "Epoch : 24, Time : 0.009168624877929688, Val_loss: 0.5118046402931213, Val_Acc: 0.8947562776957164\n",
      "Epoch : 25, Time : 0.009479045867919922, Val_loss: 0.48068758845329285, Val_Acc: 0.8973412112259971\n",
      "Epoch : 26, Time : 0.010143518447875977, Val_loss: 0.45299410820007324, Val_Acc: 0.9006646971935008\n",
      "Epoch : 27, Time : 0.009674787521362305, Val_loss: 0.4285237789154053, Val_Acc: 0.9014032496307238\n",
      "Epoch : 28, Time : 0.009622812271118164, Val_loss: 0.40687593817710876, Val_Acc: 0.904357459379616\n",
      "Epoch : 29, Time : 0.009814739227294922, Val_loss: 0.3876599669456482, Val_Acc: 0.9065731166912852\n",
      "Epoch : 30, Time : 0.009878158569335938, Val_loss: 0.37044557929039, Val_Acc: 0.9106351550960119\n",
      "Epoch : 31, Time : 0.009625434875488281, Val_loss: 0.35511544346809387, Val_Acc: 0.912850812407681\n",
      "Epoch : 32, Time : 0.010369539260864258, Val_loss: 0.3413562476634979, Val_Acc: 0.9146971935007386\n",
      "Epoch : 33, Time : 0.010181427001953125, Val_loss: 0.3290456533432007, Val_Acc: 0.9158050221565732\n",
      "Epoch : 34, Time : 0.010131359100341797, Val_loss: 0.31808027625083923, Val_Acc: 0.9183899556868538\n",
      "Epoch : 35, Time : 0.010430574417114258, Val_loss: 0.30818676948547363, Val_Acc: 0.9183899556868538\n",
      "Epoch : 36, Time : 0.01052546501159668, Val_loss: 0.29914727807044983, Val_Acc: 0.9187592319054654\n",
      "Epoch : 37, Time : 0.01086282730102539, Val_loss: 0.2909115254878998, Val_Acc: 0.9194977843426884\n",
      "Epoch : 38, Time : 0.010677814483642578, Val_loss: 0.28357774019241333, Val_Acc: 0.921344165435746\n",
      "Epoch : 39, Time : 0.010625839233398438, Val_loss: 0.2770865857601166, Val_Acc: 0.9217134416543575\n",
      "Epoch : 40, Time : 0.010767698287963867, Val_loss: 0.2712942063808441, Val_Acc: 0.9242983751846382\n",
      "Epoch : 41, Time : 0.01066732406616211, Val_loss: 0.26608532667160034, Val_Acc: 0.9250369276218612\n",
      "Epoch : 42, Time : 0.010498285293579102, Val_loss: 0.2612019181251526, Val_Acc: 0.9261447562776958\n",
      "Epoch : 43, Time : 0.010601282119750977, Val_loss: 0.25652140378952026, Val_Acc: 0.9272525849335304\n",
      "Epoch : 44, Time : 0.009957551956176758, Val_loss: 0.25175631046295166, Val_Acc: 0.9276218611521418\n",
      "Epoch : 45, Time : 0.00989079475402832, Val_loss: 0.24729102849960327, Val_Acc: 0.9287296898079764\n",
      "Epoch : 46, Time : 0.009845733642578125, Val_loss: 0.24314555525779724, Val_Acc: 0.9294682422451994\n",
      "Epoch : 47, Time : 0.010368585586547852, Val_loss: 0.23961584270000458, Val_Acc: 0.929837518463811\n",
      "Epoch : 48, Time : 0.010724782943725586, Val_loss: 0.23637153208255768, Val_Acc: 0.930576070901034\n",
      "Epoch : 49, Time : 0.010194063186645508, Val_loss: 0.23327083885669708, Val_Acc: 0.9316838995568686\n",
      "Epoch : 50, Time : 0.01052999496459961, Val_loss: 0.23034043610095978, Val_Acc: 0.9302067946824225\n",
      "Epoch : 51, Time : 0.010754823684692383, Val_loss: 0.2273978590965271, Val_Acc: 0.9309453471196456\n",
      "Epoch : 52, Time : 0.010776042938232422, Val_loss: 0.22498509287834167, Val_Acc: 0.930576070901034\n",
      "Epoch : 53, Time : 0.010624408721923828, Val_loss: 0.22294797003269196, Val_Acc: 0.9309453471196456\n",
      "Epoch : 54, Time : 0.010591506958007812, Val_loss: 0.2207922488451004, Val_Acc: 0.9320531757754801\n",
      "Epoch : 55, Time : 0.010709762573242188, Val_loss: 0.21868927776813507, Val_Acc: 0.9309453471196456\n",
      "Epoch : 56, Time : 0.010739326477050781, Val_loss: 0.21656130254268646, Val_Acc: 0.9316838995568686\n",
      "Epoch : 57, Time : 0.010805606842041016, Val_loss: 0.21416619420051575, Val_Acc: 0.9331610044313147\n",
      "Epoch : 58, Time : 0.010833024978637695, Val_loss: 0.21175919473171234, Val_Acc: 0.9335302806499262\n",
      "Epoch : 59, Time : 0.010418176651000977, Val_loss: 0.20980589091777802, Val_Acc: 0.9357459379615953\n",
      "Epoch : 60, Time : 0.011254072189331055, Val_loss: 0.20831064879894257, Val_Acc: 0.9350073855243723\n",
      "Epoch : 61, Time : 0.010560750961303711, Val_loss: 0.20722177624702454, Val_Acc: 0.9335302806499262\n",
      "Epoch : 62, Time : 0.010733366012573242, Val_loss: 0.2055164873600006, Val_Acc: 0.9338995568685378\n",
      "Epoch : 63, Time : 0.010703325271606445, Val_loss: 0.20346008241176605, Val_Acc: 0.9361152141802068\n",
      "Epoch : 64, Time : 0.011145830154418945, Val_loss: 0.20159515738487244, Val_Acc: 0.9368537666174299\n",
      "Epoch : 65, Time : 0.011009454727172852, Val_loss: 0.20031368732452393, Val_Acc: 0.9375923190546529\n",
      "Epoch : 66, Time : 0.010616302490234375, Val_loss: 0.19958710670471191, Val_Acc: 0.939069423929099\n",
      "Epoch : 67, Time : 0.010747671127319336, Val_loss: 0.19877126812934875, Val_Acc: 0.9401772525849336\n",
      "Epoch : 68, Time : 0.010426044464111328, Val_loss: 0.19735215604305267, Val_Acc: 0.9409158050221567\n",
      "Epoch : 69, Time : 0.010654211044311523, Val_loss: 0.19541452825069427, Val_Acc: 0.9405465288035452\n",
      "Epoch : 70, Time : 0.01037287712097168, Val_loss: 0.19361662864685059, Val_Acc: 0.9412850812407682\n",
      "Epoch : 71, Time : 0.010442495346069336, Val_loss: 0.19200780987739563, Val_Acc: 0.9427621861152142\n",
      "Epoch : 72, Time : 0.01057124137878418, Val_loss: 0.1908625215291977, Val_Acc: 0.9427621861152142\n",
      "Epoch : 73, Time : 0.010699987411499023, Val_loss: 0.18980877101421356, Val_Acc: 0.9449778434268834\n",
      "Epoch : 74, Time : 0.010813236236572266, Val_loss: 0.18871884047985077, Val_Acc: 0.9453471196454949\n",
      "Epoch : 75, Time : 0.010741472244262695, Val_loss: 0.1877368539571762, Val_Acc: 0.946824224519941\n",
      "Epoch : 76, Time : 0.010259628295898438, Val_loss: 0.1867927461862564, Val_Acc: 0.946085672082718\n",
      "Epoch : 77, Time : 0.010616302490234375, Val_loss: 0.1859363317489624, Val_Acc: 0.9464549483013295\n",
      "Epoch : 78, Time : 0.010535955429077148, Val_loss: 0.18525047600269318, Val_Acc: 0.946085672082718\n",
      "Epoch : 79, Time : 0.010660648345947266, Val_loss: 0.18460708856582642, Val_Acc: 0.9457163958641064\n",
      "Epoch : 80, Time : 0.010604143142700195, Val_loss: 0.18404753506183624, Val_Acc: 0.9449778434268834\n",
      "Epoch : 81, Time : 0.010508298873901367, Val_loss: 0.18335044384002686, Val_Acc: 0.9446085672082718\n",
      "Epoch : 82, Time : 0.010522603988647461, Val_loss: 0.18255215883255005, Val_Acc: 0.9453471196454949\n",
      "Epoch : 83, Time : 0.010620594024658203, Val_loss: 0.1818065345287323, Val_Acc: 0.9449778434268834\n",
      "Epoch : 84, Time : 0.010561704635620117, Val_loss: 0.18090753257274628, Val_Acc: 0.9464549483013295\n",
      "Epoch : 85, Time : 0.010703802108764648, Val_loss: 0.17995896935462952, Val_Acc: 0.9464549483013295\n",
      "Epoch : 86, Time : 0.010646820068359375, Val_loss: 0.17907100915908813, Val_Acc: 0.9471935007385525\n",
      "Epoch : 87, Time : 0.010541200637817383, Val_loss: 0.17807386815547943, Val_Acc: 0.9475627769571641\n",
      "Epoch : 88, Time : 0.010612726211547852, Val_loss: 0.17718718945980072, Val_Acc: 0.9479320531757756\n",
      "Epoch : 89, Time : 0.01061105728149414, Val_loss: 0.17639979720115662, Val_Acc: 0.948301329394387\n",
      "Epoch : 90, Time : 0.010531187057495117, Val_loss: 0.17572662234306335, Val_Acc: 0.9497784342688331\n",
      "Epoch : 91, Time : 0.010491132736206055, Val_loss: 0.1751980483531952, Val_Acc: 0.9494091580502216\n",
      "Epoch : 92, Time : 0.010512590408325195, Val_loss: 0.17468474805355072, Val_Acc: 0.9486706056129985\n",
      "Epoch : 93, Time : 0.010647773742675781, Val_loss: 0.17361168563365936, Val_Acc: 0.9490398818316101\n",
      "Epoch : 94, Time : 0.01045846939086914, Val_loss: 0.17219996452331543, Val_Acc: 0.9494091580502216\n",
      "Epoch : 95, Time : 0.010699748992919922, Val_loss: 0.17099112272262573, Val_Acc: 0.9497784342688331\n",
      "Epoch : 96, Time : 0.010566949844360352, Val_loss: 0.1703447550535202, Val_Acc: 0.9479320531757756\n",
      "Epoch : 97, Time : 0.010586261749267578, Val_loss: 0.16991929709911346, Val_Acc: 0.9497784342688331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 98, Time : 0.010676860809326172, Val_loss: 0.16941550374031067, Val_Acc: 0.9486706056129985\n",
      "Epoch : 99, Time : 0.010644197463989258, Val_loss: 0.16885775327682495, Val_Acc: 0.948301329394387\n",
      "Epoch : 100, Time : 0.010524988174438477, Val_loss: 0.16828861832618713, Val_Acc: 0.9490398818316101\n",
      "Epoch : 101, Time : 0.010691642761230469, Val_loss: 0.16793623566627502, Val_Acc: 0.9497784342688331\n",
      "Epoch : 102, Time : 0.010489702224731445, Val_loss: 0.16775746643543243, Val_Acc: 0.9490398818316101\n",
      "Epoch : 103, Time : 0.010762929916381836, Val_loss: 0.16736119985580444, Val_Acc: 0.9494091580502216\n",
      "Epoch : 104, Time : 0.010607004165649414, Val_loss: 0.16690218448638916, Val_Acc: 0.9494091580502216\n",
      "Epoch : 105, Time : 0.010873794555664062, Val_loss: 0.16628219187259674, Val_Acc: 0.9490398818316101\n",
      "Epoch : 106, Time : 0.010739326477050781, Val_loss: 0.1653369963169098, Val_Acc: 0.9486706056129985\n",
      "Epoch : 107, Time : 0.010054826736450195, Val_loss: 0.16434553265571594, Val_Acc: 0.948301329394387\n",
      "Epoch : 108, Time : 0.010355234146118164, Val_loss: 0.16290244460105896, Val_Acc: 0.9479320531757756\n",
      "Epoch : 109, Time : 0.010601997375488281, Val_loss: 0.16187769174575806, Val_Acc: 0.9501477104874446\n",
      "Epoch : 110, Time : 0.010253190994262695, Val_loss: 0.16125664114952087, Val_Acc: 0.9508862629246677\n",
      "Epoch : 111, Time : 0.010597467422485352, Val_loss: 0.16111980378627777, Val_Acc: 0.9508862629246677\n",
      "Epoch : 112, Time : 0.010526657104492188, Val_loss: 0.1607053428888321, Val_Acc: 0.9497784342688331\n",
      "Epoch : 113, Time : 0.010553836822509766, Val_loss: 0.15986797213554382, Val_Acc: 0.9497784342688331\n",
      "Epoch : 114, Time : 0.010412454605102539, Val_loss: 0.15900219976902008, Val_Acc: 0.9516248153618908\n",
      "Epoch : 115, Time : 0.010682344436645508, Val_loss: 0.15863382816314697, Val_Acc: 0.9523633677991138\n",
      "Epoch : 116, Time : 0.010506868362426758, Val_loss: 0.15834127366542816, Val_Acc: 0.9516248153618908\n",
      "Epoch : 117, Time : 0.011287450790405273, Val_loss: 0.15797379612922668, Val_Acc: 0.9519940915805023\n",
      "Epoch : 118, Time : 0.010501623153686523, Val_loss: 0.15748687088489532, Val_Acc: 0.9512555391432792\n",
      "Epoch : 119, Time : 0.010588645935058594, Val_loss: 0.15675684809684753, Val_Acc: 0.9516248153618908\n",
      "Epoch : 120, Time : 0.010541915893554688, Val_loss: 0.15627223253250122, Val_Acc: 0.9527326440177253\n",
      "Epoch : 121, Time : 0.010435819625854492, Val_loss: 0.15572308003902435, Val_Acc: 0.9527326440177253\n",
      "Epoch : 122, Time : 0.010437250137329102, Val_loss: 0.1551031768321991, Val_Acc: 0.9531019202363369\n",
      "Epoch : 123, Time : 0.010521173477172852, Val_loss: 0.15451304614543915, Val_Acc: 0.9523633677991138\n",
      "Epoch : 124, Time : 0.010810613632202148, Val_loss: 0.15409889817237854, Val_Acc: 0.9519940915805023\n",
      "Epoch : 125, Time : 0.010350704193115234, Val_loss: 0.15359431505203247, Val_Acc: 0.9523633677991138\n",
      "Epoch : 126, Time : 0.01061701774597168, Val_loss: 0.15315905213356018, Val_Acc: 0.954579025110783\n",
      "Epoch : 127, Time : 0.010781526565551758, Val_loss: 0.1530485302209854, Val_Acc: 0.9534711964549484\n",
      "Epoch : 128, Time : 0.010285139083862305, Val_loss: 0.15273435413837433, Val_Acc: 0.9542097488921714\n",
      "Epoch : 129, Time : 0.010671854019165039, Val_loss: 0.15208742022514343, Val_Acc: 0.9556868537666174\n",
      "Epoch : 130, Time : 0.010656118392944336, Val_loss: 0.1516668051481247, Val_Acc: 0.9531019202363369\n",
      "Epoch : 131, Time : 0.010308504104614258, Val_loss: 0.15093742311000824, Val_Acc: 0.9534711964549484\n",
      "Epoch : 132, Time : 0.010686874389648438, Val_loss: 0.15024064481258392, Val_Acc: 0.9534711964549484\n",
      "Epoch : 133, Time : 0.010675668716430664, Val_loss: 0.1496603935956955, Val_Acc: 0.9527326440177253\n",
      "Epoch : 134, Time : 0.010315895080566406, Val_loss: 0.1490071713924408, Val_Acc: 0.9531019202363369\n",
      "Epoch : 135, Time : 0.010851383209228516, Val_loss: 0.14880456030368805, Val_Acc: 0.9564254062038405\n",
      "Epoch : 136, Time : 0.010800361633300781, Val_loss: 0.14857636392116547, Val_Acc: 0.9564254062038405\n",
      "Epoch : 137, Time : 0.010682344436645508, Val_loss: 0.14821363985538483, Val_Acc: 0.956794682422452\n",
      "Epoch : 138, Time : 0.010727643966674805, Val_loss: 0.14768269658088684, Val_Acc: 0.9549483013293945\n",
      "Epoch : 139, Time : 0.010689973831176758, Val_loss: 0.147343710064888, Val_Acc: 0.954579025110783\n",
      "Epoch : 140, Time : 0.010607719421386719, Val_loss: 0.14677627384662628, Val_Acc: 0.9527326440177253\n",
      "Epoch : 141, Time : 0.010551691055297852, Val_loss: 0.14690938591957092, Val_Acc: 0.954579025110783\n",
      "Epoch : 142, Time : 0.01081228256225586, Val_loss: 0.14670205116271973, Val_Acc: 0.9549483013293945\n",
      "Epoch : 143, Time : 0.010802507400512695, Val_loss: 0.14633296430110931, Val_Acc: 0.9542097488921714\n",
      "Epoch : 144, Time : 0.010728597640991211, Val_loss: 0.14585871994495392, Val_Acc: 0.9538404726735599\n",
      "Epoch : 145, Time : 0.010678768157958984, Val_loss: 0.14519144594669342, Val_Acc: 0.9534711964549484\n",
      "Epoch : 146, Time : 0.010854482650756836, Val_loss: 0.14527633786201477, Val_Acc: 0.954579025110783\n",
      "Epoch : 147, Time : 0.010422706604003906, Val_loss: 0.14548803865909576, Val_Acc: 0.9538404726735599\n",
      "Epoch : 148, Time : 0.010714292526245117, Val_loss: 0.14455443620681763, Val_Acc: 0.9538404726735599\n",
      "Epoch : 149, Time : 0.010602951049804688, Val_loss: 0.14391085505485535, Val_Acc: 0.954579025110783\n",
      "Epoch : 150, Time : 0.0106658935546875, Val_loss: 0.14403021335601807, Val_Acc: 0.9542097488921714\n",
      "Epoch : 151, Time : 0.010502815246582031, Val_loss: 0.1437310129404068, Val_Acc: 0.9542097488921714\n",
      "Epoch : 152, Time : 0.010564565658569336, Val_loss: 0.14310182631015778, Val_Acc: 0.9538404726735599\n",
      "Epoch : 153, Time : 0.010805130004882812, Val_loss: 0.14248345792293549, Val_Acc: 0.955317577548006\n",
      "Epoch : 154, Time : 0.010630369186401367, Val_loss: 0.14255738258361816, Val_Acc: 0.956794682422452\n",
      "Epoch : 155, Time : 0.010675430297851562, Val_loss: 0.14261233806610107, Val_Acc: 0.9571639586410636\n",
      "Epoch : 156, Time : 0.010696887969970703, Val_loss: 0.1419549286365509, Val_Acc: 0.9593796159527327\n",
      "Epoch : 157, Time : 0.010341882705688477, Val_loss: 0.141695037484169, Val_Acc: 0.9586410635155097\n",
      "Epoch : 158, Time : 0.010760307312011719, Val_loss: 0.1416197121143341, Val_Acc: 0.9575332348596751\n",
      "Epoch : 159, Time : 0.010683774948120117, Val_loss: 0.14122559130191803, Val_Acc: 0.956794682422452\n",
      "Epoch : 160, Time : 0.010607719421386719, Val_loss: 0.14056000113487244, Val_Acc: 0.9582717872968981\n",
      "Epoch : 161, Time : 0.010551691055297852, Val_loss: 0.1401771903038025, Val_Acc: 0.9604874446085673\n",
      "Epoch : 162, Time : 0.010689020156860352, Val_loss: 0.1403554230928421, Val_Acc: 0.9597488921713442\n",
      "Epoch : 163, Time : 0.010420799255371094, Val_loss: 0.14084741473197937, Val_Acc: 0.9597488921713442\n",
      "Epoch : 164, Time : 0.010818243026733398, Val_loss: 0.14085613191127777, Val_Acc: 0.9590103397341212\n",
      "Epoch : 165, Time : 0.010670900344848633, Val_loss: 0.1397734433412552, Val_Acc: 0.9590103397341212\n",
      "Epoch : 166, Time : 0.010734796524047852, Val_loss: 0.1384236216545105, Val_Acc: 0.9608567208271788\n",
      "Epoch : 167, Time : 0.010610818862915039, Val_loss: 0.13827882707118988, Val_Acc: 0.9597488921713442\n",
      "Epoch : 168, Time : 0.010468244552612305, Val_loss: 0.13781793415546417, Val_Acc: 0.9579025110782866\n",
      "Epoch : 169, Time : 0.010698556900024414, Val_loss: 0.13735438883304596, Val_Acc: 0.9556868537666174\n",
      "Epoch : 170, Time : 0.010533332824707031, Val_loss: 0.13728196918964386, Val_Acc: 0.954579025110783\n",
      "Epoch : 171, Time : 0.01058340072631836, Val_loss: 0.13796472549438477, Val_Acc: 0.956794682422452\n",
      "Epoch : 172, Time : 0.010582685470581055, Val_loss: 0.13797743618488312, Val_Acc: 0.956794682422452\n",
      "Epoch : 173, Time : 0.010815143585205078, Val_loss: 0.13739538192749023, Val_Acc: 0.956056129985229\n",
      "Epoch : 174, Time : 0.01127171516418457, Val_loss: 0.13673114776611328, Val_Acc: 0.9582717872968981\n",
      "Epoch : 175, Time : 0.010939836502075195, Val_loss: 0.13728894293308258, Val_Acc: 0.9601181683899558\n",
      "Epoch : 176, Time : 0.010451555252075195, Val_loss: 0.13859431445598602, Val_Acc: 0.9593796159527327\n",
      "Epoch : 177, Time : 0.010642766952514648, Val_loss: 0.1388334035873413, Val_Acc: 0.9601181683899558\n",
      "Epoch : 178, Time : 0.010512113571166992, Val_loss: 0.13816896080970764, Val_Acc: 0.9601181683899558\n",
      "Epoch : 179, Time : 0.010847330093383789, Val_loss: 0.1368647813796997, Val_Acc: 0.9582717872968981\n",
      "Epoch : 180, Time : 0.010720491409301758, Val_loss: 0.13605110347270966, Val_Acc: 0.9593796159527327\n",
      "Epoch : 181, Time : 0.010873079299926758, Val_loss: 0.13649341464042664, Val_Acc: 0.9615952732644019\n",
      "Epoch : 182, Time : 0.010912418365478516, Val_loss: 0.1371191143989563, Val_Acc: 0.9586410635155097\n",
      "Epoch : 183, Time : 0.010457277297973633, Val_loss: 0.13647975027561188, Val_Acc: 0.9582717872968981\n",
      "Epoch : 184, Time : 0.01089787483215332, Val_loss: 0.13434761762619019, Val_Acc: 0.9579025110782866\n",
      "Epoch : 185, Time : 0.010701417922973633, Val_loss: 0.13270819187164307, Val_Acc: 0.956056129985229\n",
      "Epoch : 186, Time : 0.010979413986206055, Val_loss: 0.1324145644903183, Val_Acc: 0.9571639586410636\n",
      "Epoch : 187, Time : 0.010692834854125977, Val_loss: 0.1323307603597641, Val_Acc: 0.9612259970457904\n",
      "Epoch : 188, Time : 0.010979652404785156, Val_loss: 0.13228526711463928, Val_Acc: 0.9619645494830134\n",
      "Epoch : 189, Time : 0.01101064682006836, Val_loss: 0.13214167952537537, Val_Acc: 0.9608567208271788\n",
      "Epoch : 190, Time : 0.010213851928710938, Val_loss: 0.13186384737491608, Val_Acc: 0.9604874446085673\n",
      "Epoch : 191, Time : 0.010495901107788086, Val_loss: 0.13158676028251648, Val_Acc: 0.9593796159527327\n",
      "Epoch : 192, Time : 0.010637998580932617, Val_loss: 0.13161401450634003, Val_Acc: 0.9601181683899558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 193, Time : 0.010562658309936523, Val_loss: 0.13123181462287903, Val_Acc: 0.9612259970457904\n",
      "Epoch : 194, Time : 0.010937929153442383, Val_loss: 0.1312025487422943, Val_Acc: 0.9615952732644019\n",
      "Epoch : 195, Time : 0.010747671127319336, Val_loss: 0.1312369853258133, Val_Acc: 0.9615952732644019\n",
      "Epoch : 196, Time : 0.010840415954589844, Val_loss: 0.13090941309928894, Val_Acc: 0.9634416543574594\n",
      "Epoch : 197, Time : 0.010615110397338867, Val_loss: 0.13034628331661224, Val_Acc: 0.9627031019202364\n",
      "Epoch : 198, Time : 0.010836601257324219, Val_loss: 0.12989135086536407, Val_Acc: 0.9623338257016248\n",
      "Epoch : 199, Time : 0.010625362396240234, Val_loss: 0.129765585064888, Val_Acc: 0.9623338257016248\n",
      "0.129765585064888 0.9623338257016248\n",
      "finish------------\n"
     ]
    }
   ],
   "source": [
    "def test(model, idx_test):\n",
    "    model.eval()\n",
    "    output = model(features,L)\n",
    "    loss_test = F.nll_loss(output[idx_test],labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test],labels[idx_test])\n",
    "    print(loss_test.item(), acc_test.item())\n",
    "\n",
    "def train(model, idx_train, idx_val):\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features,L)\n",
    "        loss_train = F.nll_loss(output[idx_train],labels[idx_train])\n",
    "        acc_train = accuracy(output[idx_train],labels[idx_train])\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        output = model(features,L)\n",
    "        loss_val = F.nll_loss(output[idx_val],labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val],labels[idx_val])\n",
    "        #label = labels[idx_val].cpu()\n",
    "        #output = output[idx_val].max(1)[1].type_as(labels).cpu()\n",
    "        #f1_val  = f1_score(label.numpy(),output.numpy(), average=\"micro\")\n",
    "        take_time = time.time()-t\n",
    "        print(\"Epoch : {}, Time : {}, Val_loss: {}, Val_Acc: {}\".format(epoch, take_time, loss_val.item(), acc_val.item()))\n",
    "    \n",
    "\n",
    "print(\"start training----\")\n",
    "for i in range(1):\n",
    "    print(i)\n",
    "    train(model, idx_train, idx_valid)\n",
    "    test(model, idx_test)\n",
    "print('finish------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training GCN with all data in Cora.\n",
    "Epoch : 199, Time : 0.010625362396240234, Val_loss: 0.129765585064888, Val_Acc: 0.9623338257016248\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save and Load Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"gcn_identity_features_photo.pkt\"\n",
    "torch.save(model, model_path)\n",
    "model = torch.load(model_path)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"gcn_identity_features_photo_state.pkt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"gcn_photo.pkt\"\n",
    "model = torch.load(model_path)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hook Model Definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import skcuda.linalg as sklin\n",
    "from layer import GCN_layer, BiGCN1_layer, BiGCN2_layer, BiGCN3_layer \n",
    "\n",
    "'''\n",
    "GCN_layer(ind,outd,bias=True)\n",
    "BiGCN1_layer(ind,outd,p,bias=True,beta=True,A2='cos_A2',n_iter=2)  \n",
    "BiGCN2_layer(ind,outd,bias=True, beta=True, A2=None)\n",
    "BiGCN3_layer(ind,outd,bias=True, beta=True, A2=None)\n",
    "--A2 cos_A2/learn_A2/None\n",
    "'''\n",
    "\n",
    "class GCN_hook(nn.Module):\n",
    "    def __init__(self, num_feature,num_hidden,num_class,dropout,bias=True):\n",
    "        super(GCN_hook,self).__init__()\n",
    "\n",
    "        self.gc1 = GCN_layer(num_feature, num_hidden)\n",
    "        self.gc2 = GCN_layer(num_hidden, num_class)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x1 = F.dropout(x, self.dropout, training=self.training)\n",
    "        x2 = self.gc2(x1, adj)\n",
    "        return F.log_softmax(x2, dim=1), x1\n",
    "\n",
    "class GCN_3layer_hook(nn.Module):\n",
    "    def __init__(self, num_feature,num_hidden,num_class,dropout,bias=True):\n",
    "        super(GCN_3layer_hook,self).__init__()\n",
    "\n",
    "        self.gc1 = GCN_layer(num_feature, num_hidden)\n",
    "        self.gc2 = GCN_layer(num_hidden, num_hidden)\n",
    "        self.gc3 = GCN_layer(num_hidden, num_class)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x1 = F.dropout(x, self.dropout, training=self.training)\n",
    "        x2 = F.relu(self.gc2(x1, adj))\n",
    "        x2 = F.dropout(x2, self.dropout, training=self.training)\n",
    "        x3 = self.gc3(x2, adj)\n",
    "        return F.log_softmax(x3, dim=1), x1,x2\n",
    "    \n",
    "class GCN_nlayer_hook(nn.Module):\n",
    "    def __init__(self, num_feature,num_hidden,num_class,dropout,layer_num=2,bias=True):\n",
    "        super(GCN_nlayer_hook,self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.gc_input = GCN_layer(num_feature, num_hidden)\n",
    "        if layer_num > 2:\n",
    "            self.gc_inner = nn.ModuleList([GCN_layer(num_hidden, num_hidden) for i in range(layer_num-2)])        \n",
    "        self.gc_output = GCN_layer(num_hidden, num_class)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        inner_state = []\n",
    "        x = F.relu(self.gc_input(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        inner_state.append(x)\n",
    "        if self.layer_num > 2:\n",
    "            for i in range(self.layer_num - 2):\n",
    "                x = self.gc_inner[i](x,adj)\n",
    "                x = F.dropout(x, self.dropout, training=self.training)\n",
    "                inner_state.append(x)\n",
    "        x = self.gc_output(x, adj)\n",
    "        return F.log_softmax(x, dim=1), inner_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"gcn_cora_%dlayer_symnorm_state.pkt\".format(6)\n",
    "#torch.save(model.state_dict(), model_path)\n",
    "args_input = [1433,16,7,0.5]\n",
    "kwargs = {\n",
    "    \"layer_num\": 6,\n",
    "    \"bias\": True\n",
    "}\n",
    "model = GCN_nlayer_hook(*args_input,**kwargs).cuda()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "output, inner_state = model(features,L)\n",
    "loss_test = F.nll_loss(output[idx_test],labels[idx_test])\n",
    "acc_test = accuracy(output[idx_test],labels[idx_test])\n",
    "print(loss_test.item(), acc_test.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"gcn_identity_features_cora.pkt\"\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "output = model(features,L)\n",
    "loss_test = F.nll_loss(output[idx_test],labels[idx_test])\n",
    "acc_test = accuracy(output[idx_test],labels[idx_test])\n",
    "print(loss_test.item(), acc_test.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hidden State Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "def dimension_reduction(input_array):\n",
    "    start_time = time.time()\n",
    "    X = np.array(input_array)    \n",
    "    X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "    print(time.time() - start_time)\n",
    "    return X_embedded\n",
    "def visualize(embedded_array,labels):\n",
    "    \n",
    "    #sns.set_palette(sns.color_palette(\"Paired\"))\n",
    "    X = np.array(embedded_array)\n",
    "    labels = np.array(labels)\n",
    "    labels = np.expand_dims(labels, axis=1)\n",
    "    data = np.concatenate((X, labels), axis=1)\n",
    "    df = pd.DataFrame(data, columns=[\"x\", \"y\",\"Labels\"])\n",
    "    # Create an array with the colors you want to use\n",
    "    colors = [\"#FF0B04\", \"#4374B3\"]\n",
    "    # Set your custom color palette\n",
    "    customPalette = sns.set_palette(sns.color_palette(colors))\n",
    "    ax = sns.scatterplot(x=\"x\", y=\"y\",hue=\"Labels\", data=df, palette=\"Set1\", legend=False)\n",
    "    \n",
    "current_palette = sns.color_palette()\n",
    "sns.palplot(current_palette)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output\n",
    "input_array = output.cpu().detach().numpy()\n",
    "labels_array = labels.cpu().detach().numpy()\n",
    "embedded_array = dimension_reduction(input_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input\n",
    "features_array = features.cpu().detach().numpy()\n",
    "features_embeddded_array = dimension_reduction(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layer2\n",
    "inner_embedded_array = []\n",
    "labels_array = labels.cpu().detach().numpy()\n",
    "\n",
    "for i in range(len(inner_state)):\n",
    "    layer2 = inner_state[i]\n",
    "    layer2_array = layer2.cpu().detach().numpy()\n",
    "    layer2_embeddded_array = dimension_reduction(layer2_array)\n",
    "    inner_embedded_array.append(layer2_embeddded_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layer3\n",
    "layer3_array = layer3.cpu().detach().numpy()\n",
    "layer3_embeddded_array = dimension_reduction(layer3_array)\n",
    "labels_array = labels.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(embedded_array, labels.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(inner_embedded_array[3], labels.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer2_embeddded_array)\n",
    "import pickle as pkl\n",
    "with open(\"cora_tsne_hidden.pkt\",\"wb\") as f:\n",
    "    pkl.dump(layer2_embeddded_array, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
